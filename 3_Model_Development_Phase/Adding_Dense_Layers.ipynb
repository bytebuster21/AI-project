{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjwJKIE6wYLszqirr75HVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bytebuster21/AI-project/blob/main/Adding_Dense_Layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "zZhT7jenATPu",
        "outputId": "5cfccbdc-f067-4c53-e725-afc77595daac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully!\n",
            "Creating dummy dataset for demonstration...\n",
            "Dummy dataset created at /content/train and /content/test\n",
            "\n",
            "Loading images using ImageDataGenerator...\n",
            "Found 348 images belonging to 3 classes.\n",
            "Found 153 images belonging to 3 classes.\n",
            "Found 348 training images belonging to 3 classes.\n",
            "Found 153 test images belonging to 3 classes.\n",
            "Class Names: ['class_A', 'class_B', 'class_C']\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "--- Complete Model Summary (VGG16 with Custom Dense Layers) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41472\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m10,617,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41472</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,617,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,365,059\u001b[0m (96.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,365,059</span> (96.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,650,371\u001b[0m (40.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,650,371</span> (40.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training (Commented out - uncomment to run) ---\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16 # Import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")\n",
        "\n",
        "# --- 2. Define your dataset paths ---\n",
        "trainpath = '/content/train'\n",
        "testpath = '/content/test'\n",
        "\n",
        "if not os.path.exists(trainpath):\n",
        "    print(\"Creating dummy dataset for demonstration...\")\n",
        "    os.makedirs(os.path.join(trainpath, 'class_A'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(trainpath, 'class_B'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(trainpath, 'class_C'), exist_ok=True)\n",
        "\n",
        "    os.makedirs(os.path.join(testpath, 'class_A'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(testpath, 'class_B'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(testpath, 'class_C'), exist_ok=True)\n",
        "\n",
        "    from PIL import Image\n",
        "    def create_dummy_image(path, color, size=(299, 299)):\n",
        "        img = Image.new('RGB', size, color=color)\n",
        "        img.save(path)\n",
        "\n",
        "    # Create dummy training images\n",
        "    for i in range(116): # 116 * 3 classes = 348\n",
        "        create_dummy_image(os.path.join(trainpath, 'class_A', f'imgA_{i:03d}.png'), (255, 100, 100))\n",
        "        create_dummy_image(os.path.join(trainpath, 'class_B', f'imgB_{i:03d}.png'), (100, 255, 100))\n",
        "        create_dummy_image(os.path.join(trainpath, 'class_C', f'imgC_{i:03d}.png'), (100, 100, 255))\n",
        "\n",
        "    # Create dummy testing images\n",
        "    for i in range(51): # 51 * 3 classes = 153\n",
        "        create_dummy_image(os.path.join(testpath, 'class_A', f'testA_{i:03d}.png'), (200, 50, 50))\n",
        "        create_dummy_image(os.path.join(testpath, 'class_B', f'testB_{i:03d}.png'), (50, 200, 50))\n",
        "        create_dummy_image(os.path.join(testpath, 'class_C', f'testC_{i:03d}.png'), (50, 50, 200))\n",
        "\n",
        "    print(f\"Dummy dataset created at {trainpath} and {testpath}\")\n",
        "else:\n",
        "    print(f\"Using existing dataset at {trainpath} and {testpath}\")\n",
        "\n",
        "\n",
        "# --- 4. Configure ImageDataGenerator instances ---\n",
        "# VGG16 can accept various sizes, 299x299 is specified for this setup\n",
        "TARGET_SIZE = (299, 299)\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "# Training Data Generator (with augmentation)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Test Data Generator (ONLY rescaling)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# --- 5. Flow Images from Directories ---\n",
        "print(\"\\nLoading images using ImageDataGenerator...\")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    trainpath,\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    testpath,\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Get class names and number of classes from the generator\n",
        "num_classes = train_generator.num_classes\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "print(f\"Found {train_generator.samples} training images belonging to {num_classes} classes.\")\n",
        "print(f\"Found {test_generator.samples} test images belonging to {num_classes} classes.\")\n",
        "print(\"Class Names:\", class_names)\n",
        "\n",
        "\n",
        "# --- 6. Build the Model using VGG16 as a Pre-trained Base (Feature Extractor) and Add Custom Layers ---\n",
        "\n",
        "# Load the VGG16 model with pre-trained ImageNet weights.\n",
        "# `include_top=False` means we are not including the ImageNet classification layers\n",
        "# at the top of the network, as we will add our own for our specific classification task.\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',        # Use weights pre-trained on ImageNet\n",
        "    include_top=False,         # Exclude the classifier layers\n",
        "    input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3) # Define the input image shape\n",
        ")\n",
        "\n",
        "# Freeze the layers of the base model.\n",
        "# This is crucial to use VGG16 as a fixed feature extractor.\n",
        "# It ensures that the pre-trained weights of all five convolution blocks\n",
        "# don’t get updated during the training of the new custom layers.\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model by adding custom Dense layers on top of the VGG16 base.\n",
        "model = Sequential([\n",
        "    base_model, # Add the pre-trained VGG16 convolutional base\n",
        "    Flatten(),  # Flatten the output from the convolutional layers into a 1D vector.\n",
        "                # This layer does not affect the batch size; it only reshapes the feature map.\n",
        "    Dense(256, activation='relu'), # First custom Dense layer to learn high-level features\n",
        "    Dropout(0.5), # Dropout for regularization to prevent overfitting\n",
        "    Dense(128, activation='relu'), # Second custom Dense layer for further feature learning\n",
        "    Dropout(0.4), # Second Dropout layer\n",
        "    Dense(num_classes, activation='softmax') # Output layer with 'softmax' for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "# Use the Adam optimizer and categorical_crossentropy loss for multi-class classification.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the summary of the complete model, including the base and custom layers.\n",
        "print(\"\\n--- Complete Model Summary (VGG16 with Custom Dense Layers) ---\")\n",
        "model.summary()\n",
        "\n",
        "# --- 7. Train the Model (Uncomment to run) ---\n",
        "print(\"\\n--- Starting Model Training (Commented out - uncomment to run) ---\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKTphTTAAYiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}