{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUpLctPPL5OXwEj2Ls34dW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bytebuster21/AI-project/blob/main/Configure_the_Learning_Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "NfatnIwCA6-J",
        "outputId": "83d5a6ed-79d9-4537-8893-2ea0f65dcd48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully!\n",
            "Creating dummy dataset for demonstration...\n",
            "Dummy dataset created at /content/train and /content/test\n",
            "\n",
            "Loading images using ImageDataGenerator...\n",
            "Found 348 images belonging to 3 classes.\n",
            "Found 153 images belonging to 3 classes.\n",
            "Found 348 training images belonging to 3 classes.\n",
            "Found 153 test images belonging to 3 classes.\n",
            "Class Names: ['class_A', 'class_B', 'class_C']\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "--- Complete Model Summary (VGG16 with Custom Dense Layers) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41472\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m10,617,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41472</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,617,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,365,059\u001b[0m (96.76 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,365,059</span> (96.76 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,650,371\u001b[0m (40.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,650,371</span> (40.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training (Uncomment to run) ---\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop # Import common optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger # Import useful callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- 1. Mount Google Drive (if your data is there) ---\n",
        "# If your dataset is on Google Drive, run this cell first.\n",
        "# Otherwise, skip this step.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")\n",
        "\n",
        "# --- 2. Define your dataset paths ---\n",
        "trainpath = '/content/train'\n",
        "testpath = '/content/test'\n",
        "\n",
        "# IMPORTANT: Adjust these paths if your data is in Google Drive.\n",
        "# Example:\n",
        "# trainpath = '/content/drive/MyDrive/YourDatasetFolder/train'\n",
        "# testpath = '/content/drive/MyDrive/YourDatasetFolder/test'\n",
        "\n",
        "# --- 3. (Optional) Create Dummy Data for Demonstration if you don't have actual data yet ---\n",
        "# This block ensures the code runs even without your specific dataset.\n",
        "# If you have your actual images in trainpath and testpath, you can skip this.\n",
        "if not os.path.exists(trainpath):\n",
        "    print(\"Creating dummy dataset for demonstration...\")\n",
        "    os.makedirs(os.path.join(trainpath, 'class_A'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(trainpath, 'class_B'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(trainpath, 'class_C'), exist_ok=True)\n",
        "\n",
        "    os.makedirs(os.path.join(testpath, 'class_A'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(testpath, 'class_B'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(testpath, 'class_C'), exist_ok=True)\n",
        "\n",
        "    from PIL import Image\n",
        "    def create_dummy_image(path, color, size=(299, 299)):\n",
        "        img = Image.new('RGB', size, color=color)\n",
        "        img.save(path)\n",
        "\n",
        "    # Create dummy training images\n",
        "    for i in range(116): # 116 * 3 classes = 348\n",
        "        create_dummy_image(os.path.join(trainpath, 'class_A', f'imgA_{i:03d}.png'), (255, 100, 100))\n",
        "        create_dummy_image(os.path.join(trainpath, 'class_B', f'imgB_{i:03d}.png'), (100, 255, 100))\n",
        "        create_dummy_image(os.path.join(trainpath, 'class_C', f'imgC_{i:03d}.png'), (100, 100, 255))\n",
        "\n",
        "    # Create dummy testing images\n",
        "    for i in range(51): # 51 * 3 classes = 153\n",
        "        create_dummy_image(os.path.join(testpath, 'class_A', f'testA_{i:03d}.png'), (200, 50, 50))\n",
        "        create_dummy_image(os.path.join(testpath, 'class_B', f'testB_{i:03d}.png'), (50, 200, 50))\n",
        "        create_dummy_image(os.path.join(testpath, 'class_C', f'testC_{i:03d}.png'), (50, 50, 200))\n",
        "\n",
        "    print(f\"Dummy dataset created at {trainpath} and {testpath}\")\n",
        "else:\n",
        "    print(f\"Using existing dataset at {trainpath} and {testpath}\")\n",
        "\n",
        "\n",
        "# --- 4. Configure ImageDataGenerator instances ---\n",
        "TARGET_SIZE = (299, 299)\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "# Training Data Generator (with augmentation)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Test Data Generator (ONLY rescaling)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# --- 5. Flow Images from Directories ---\n",
        "print(\"\\nLoading images using ImageDataGenerator...\")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    trainpath,\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    testpath,\n",
        "    target_size=TARGET_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Get class names and number of classes from the generator\n",
        "num_classes = train_generator.num_classes\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "print(f\"Found {train_generator.samples} training images belonging to {num_classes} classes.\")\n",
        "print(f\"Found {test_generator.samples} test images belonging to {num_classes} classes.\")\n",
        "print(\"Class Names:\", class_names)\n",
        "\n",
        "\n",
        "# --- 6. Build the Model using VGG16 as a Pre-trained Base (Feature Extractor) and Add Custom Layers ---\n",
        "\n",
        "base_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "chosen_optimizer = Adam(learning_rate=0.0001)\n",
        "\n",
        "chosen_loss_function = 'categorical_crossentropy'\n",
        "\n",
        "chosen_metrics = ['accuracy']\n",
        "\n",
        "# Compile the model with the chosen configurations\n",
        "model.compile(\n",
        "    optimizer=chosen_optimizer,\n",
        "    loss=chosen_loss_function,\n",
        "    metrics=chosen_metrics\n",
        ")\n",
        "\n",
        "# Print the summary of the complete model\n",
        "print(\"\\n--- Complete Model Summary (VGG16 with Custom Dense Layers) ---\")\n",
        "model.summary()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', # Metric to monitor (e.g., validation loss)\n",
        "    patience=5,         # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True, # Restores model weights from the epoch with the best value of the monitored metric.\n",
        "    verbose=1           # Verbosity mode (0 = silent, 1 = progress bar)\n",
        ")\n",
        "\n",
        "checkpoint_dir = 'model_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True) # Create directory to save checkpoints\n",
        "checkpoint_filepath = os.path.join(checkpoint_dir, 'best_model_vgg16.h5')\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy', # Metric to monitor (e.g., validation accuracy)\n",
        "    save_best_only=True,    # Only save the model when the monitored metric improves\n",
        "    mode='max',             # 'max' for accuracy (higher is better), 'min' for loss (lower is better)\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 8.3 ReduceLROnPlateau: Reduces the learning rate when a metric has stopped improving.\n",
        "# This can help the model converge better in later stages of training.\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', # Metric to monitor\n",
        "    factor=0.2,         # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
        "    patience=3,         # Number of epochs with no improvement after which learning rate will be reduced.\n",
        "    min_lr=0.000001,    # Lower bound on the learning rate.\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 8.4 CSVLogger: Streams epoch results to a CSV file. Useful for plotting later.\n",
        "csv_logger = CSVLogger('training_log.csv', append=True)\n",
        "\n",
        "\n",
        "# Collect all desired callbacks into a list\n",
        "callbacks_list = [\n",
        "    early_stopping,\n",
        "    model_checkpoint,\n",
        "    reduce_lr,\n",
        "    csv_logger\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Model Training (Uncomment to run) ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrvpfQ2gBBn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}